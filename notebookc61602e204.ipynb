{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iezMhADyMDIi"
   },
   "source": [
    "# **Super Mario Bros. with Stable-Baseline3 PPO**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gAxwqotNsyl"
   },
   "source": [
    "\n",
    "\n",
    ">Super Mario Bros is a well-known video game title developed and published by Nintendo in the 1980s. It is one of the classical game titles that lived through the years and need no explanations. It is a 2D side-scrolling game, allowing the player to control the main character — Mario.\n",
    "\n",
    ">The gameplay involves moving Mario from left to right, surviving the villains, getting coins, and reaching the flag to clear stages. Mario would ultimately need to save the princess toadstool. These come with different reward systems, coins, villains, holes, and completion time.\n",
    "\n",
    ">The game environment was taken from the OpenAI Gym using the Nintendo Entertainment System (NES) python emulator. In this article, I will show how to implement the Reinforcement Learning algorithm using stable-baseline3 PPO algorithm using PyTorch library in an easy and simple way.\n",
    "\n",
    ">This project is broken into 3 parts:\n",
    "1.   Installation of all necessary\n",
    "2.   Pre-processing\n",
    "3.   Training of the model\n",
    "4.   Results and conclusions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">**So, Lets-a Go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs60_ee-RYUq"
   },
   "source": [
    "## Installation of all necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uv4CEf2TR72c"
   },
   "source": [
    ">Before starting any kind of installation it is important to know the version of Python we are working with.  This project has been developed mainly in Google Colab, but due to the limitations when rendering the game, the Visual Studio development environment has also been used.  \n",
    "\n",
    ">It is convenient that both environments share the same version of Python. In my case, I had different problems when loading the models due to the dissimilarity of the versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R62QfE8YRgI8",
    "outputId": "7fc0a7a8-987a-47a6-e33b-ec4ed18d530b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: python: command not found\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qWpwXFqVNgu"
   },
   "source": [
    ">As mentioned above, the OpenAI Gym environment has been used. In the case of the project, version 0.21.0 has been used. Be careful with the version of the enviroments, since some later versions are not stable with Stable-Baseline3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c_6r2E-HUZg0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.21.0\n",
      "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting numpy>=1.18.0\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616826 sha256=bf5b25fe73d39334aa05c5e00acaaed54ef2984b6a9ba1cd56fb13c09bab42ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
      "Successfully built gym\n",
      "Installing collected packages: numpy, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.0.0 gym-0.21.0 numpy-1.24.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRtGliu3VdC-"
   },
   "source": [
    ">The next step is to install the Super Mario Bros. game. [Super Mario Bros](https://pypi.org/project/gym-super-mario-bros/)\n",
    "\n",
    ">In case you are **not using Google Colab**, you will need your computer to have the C++ compiler. From my point of view, the easiest way is, to download Visual Studio Code and in the installation download its C++ compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UWIeOMx-Wors"
   },
   "outputs": [],
   "source": [
    "!pip install -qq gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxcjcCREXExZ"
   },
   "source": [
    ">In order to train the artificial intelligence, the PPO algorithm of stable-baseline3 will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wUt4doJtULoK"
   },
   "outputs": [],
   "source": [
    "!pip -qq install stable-baselines3==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.24.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU7a5nPOUlK5"
   },
   "source": [
    "## First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIzNUPC0UvG_"
   },
   "source": [
    ">Before starting the enviroment pre-processing, it is necessary to understand how the game enviroment works.\n",
    "\n",
    ">On the one hand, gym-super-mario-bros offers different types of enviroment. In this project two models will be trained, each with one type of enviroment version.  By this, we want to analyze the differences between training a model with the standard version and a model with the rectangle version.\n",
    "\n",
    ">Enviroment with standart version : ![Mario con la version estandar](data:image/png;base64,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)        \n",
    "                                                                                >Enviroment with rectangle version : ![MarioKube.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQAAAADgCAIAAABjIy8HAAADW0lEQVR4nO3dIU5cQQCA4aVZiajgCIgeggNUcARERUVFD9FDICorOAqCI1RwBASiElHbJrykkw7MLv/3WV7mTWD/zGaGfXtydf20g6p3qycAKwmANAGQJgDSBECaAEgTAGn7WQP9+DJrpDGfvq+57yrH8nt+6XnO+rtbAUgTAGkCIE0ApAmANAGQJgDSpp0DrLK13zy8b72/eP4Hn2/HBtowa996a5xZ++7HMs9ZrACkCYA0AZAmANIEQJoASBMAaSeeC/Snm6/73W53d3l6/fFx9Vx4DQIgzVsg0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQd/XOBakafq1P7/oRRVgDSBECaAEgTAGkCIE0ApAmANOcAB2rWc/RnfX/CW2UFIE0ApAmANAGQJgDSBECaAEgTAGkCIE0ApAmANAGQJgDSBECaAEjzeYAD5f/1X4cVgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBtP2ugm7NpQz3r6uHpRcenyQpAmgBIEwBpAiBNAKQJgDQBkDa8ef/S+/2j93U+wP+wApAmANIEQJoASBMAaQIgTQCkbW7qr9rvH+V8YK3zi7PVU/jL/e3D0PXLXuU/v41d/2HwevgX3gKRJgDSBECaAEgTAGkCIG1/LPv9o5wPzHVo+/1btua5dT5gBSBNAKQJgDQBkCYA0gRAmgBIO7m7PF1y4/f3v4aufzxfM0/etmWnYF7QHAJvgUgTAGkCIE0ApAmANAGQJgDSBECaAEgTAGkCIE0ApAmANAGQJgDSBECaAEgTAGkCIE0ApAmANAGQ9hsqUT13AN29RwAAAABJRU5ErkJggg==)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0v5U6jaCUs95"
   },
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "#The 1-1 specifies the map to be loaded\n",
    "STAGE_NAME = 'SuperMarioBros-1-1-v0' # Standar versión\n",
    "#STAGE_NAME = 'SuperMarioBros-1-1-v3' # Rectangle versión\n",
    "env = gym_super_mario_bros.make(STAGE_NAME) #Create the enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGm80N_DYbQ3"
   },
   "source": [
    ">The next step would be to specify the moves that our Mario could make. The enviroment brings us by default certain predefined movements, although we can create our own as we will see in the section of pre processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZhjKbt30ZdBe",
    "outputId": "a25d61af-64d0-444a-ec67-6bc931f84ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B'], ['A'], ['left']]\n",
      "Complex Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B'], ['A'], ['left'], ['left', 'A'], ['left', 'B'], ['left', 'A', 'B'], ['down'], ['up']]\n",
      "Right Only Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B']]\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "print(\"Simple Movements : \", SIMPLE_MOVEMENT)\n",
    "print(\"Complex Movements : \", COMPLEX_MOVEMENT)\n",
    "print(\"Right Only Movements : \", RIGHT_ONLY)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT) #specify the movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1zlnQFCaUPy"
   },
   "source": [
    ">With these steps we can start playing with mario bros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_DEOBfXmax8J"
   },
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "done = True\n",
    "for step in range(5000):\n",
    "    if done: # Done will be true if Mario dies in the game\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    #env.render() # If we are running the program in Colab we will need to comment the rendering of the environment. \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMOYjQMFcK0N"
   },
   "source": [
    "## Pre-procesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U-vNJmTGcTAU"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "# Import Frame Stacker Wrapper and GrayScaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "# Import Vectorization Wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhbkQuNweQN4"
   },
   "source": [
    ">This section analyzes the pre-processing that has been done to the environment. On the one hand, we have the SkipFrame function. By default, in each frame the game performs an action (a movement) and returns the reward for that action. What happens, is that to train the AI it is not necessary to make a move in each frame. That is why, the function executes the movement every X frames giving less work to do the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8cw3yGNOcc5K"
   },
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HgxgmIhe6Ht"
   },
   "source": [
    ">The second step is the re-scaling of our environment. By default the enviroment is given by 240*256 pixels. In order to optimize our model it is not necessary to have so many pixels and that is why we can rescale our enviroment to a smaller scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7_JRS2OueXBf",
    "outputId": "8f4e2cab-6276-476d-d186-912379eca9dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "state = env.reset()\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d54mB36Ue8BA"
   },
   "outputs": [],
   "source": [
    "class ResizeEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env, size):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        (oldh, oldw, oldc) = env.observation_space.shape\n",
    "        newshape = (size, size, oldc)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=newshape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        height, width, _ = self.observation_space.shape\n",
    "        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n",
    "        if frame.ndim == 2:\n",
    "            frame = frame[:,:,None]\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC0Zo-taglOy"
   },
   "source": [
    ">On the other hand, there is the environment reward function. By default,the reward function assumes the objective of the game is to move as far right as possible (increase the agent's x value), as fast as possible, without dying. To model this game, three separate variables compose the reward:\n",
    "\n",
    "\n",
    "\n",
    "1.   v: the difference in agent x values between states\n",
    "* in this case this is instantaneous velocity for the given step\n",
    "* v = x1 - x0\n",
    "* x0 is the x position before the step\n",
    "* x1 is the x position after the step\n",
    "* moving right ⇔ v > 0\n",
    "* moving left ⇔ v < 0\n",
    "* not moving ⇔ v = 0\n",
    "2. c: the difference in the game clock between frames\n",
    "* the penalty prevents the agent from standing still\n",
    "* c = c0 - c1\n",
    "* c0 is the clock reading before the step\n",
    "* c1 is the clock reading after the step\n",
    "* no clock tick ⇔ c = 0\n",
    "* clock tick ⇔ c < 0\n",
    "3. d: death penalty that penalizes the agent for dying in a state\n",
    "* this penalty encourages the agent to avoid death\n",
    "* alive ⇔ d = 0\n",
    "* dead ⇔ d = -15\n",
    "\n",
    ">r = v + c + d\n",
    "\n",
    ">The reward is clipped into the range (-15, 15).\n",
    "It is in this function that we can start to try new things, such as creating an AI that prioritizes obtaining coins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GXK7ljdif9xG"
   },
   "outputs": [],
   "source": [
    "class CustomRewardAndDoneEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(CustomRewardAndDoneEnv, self).__init__(env)\n",
    "        self.current_score = 0\n",
    "        self.current_x = 0\n",
    "        self.current_x_count = 0\n",
    "        self.max_x = 0\n",
    "    def reset(self, **kwargs):\n",
    "        self.current_score = 0\n",
    "        self.current_x = 0\n",
    "        self.current_x_count = 0\n",
    "        self.max_x = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        reward += max(0, info['x_pos'] - self.max_x)\n",
    "        if (info['x_pos'] - self.current_x) == 0:\n",
    "            self.current_x_count += 1\n",
    "        else:\n",
    "            self.current_x_count = 0\n",
    "        if info[\"flag_get\"]:\n",
    "            reward += 500\n",
    "            done = True\n",
    "            print(\"GOAL\")\n",
    "        if info[\"life\"] < 2:\n",
    "            reward -= 500\n",
    "            done = True\n",
    "        self.current_score = info[\"score\"]\n",
    "        self.max_x = max(self.max_x, self.current_x)\n",
    "        self.current_x = info[\"x_pos\"]\n",
    "        return state, reward / 10., done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEeD3hH7h05q"
   },
   "source": [
    ">By default the environment is composed of the RGB room. This data is unnecessary when training our model and we will get better results if we convert our game to a grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DB40N-PwilSy",
    "outputId": "b2b26bff-d70c-4ef2-ed08-6fbb0588a5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB scale :  (240, 256, 3)\n",
      "Gray scale: (240, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "state = env.reset()\n",
    "print(\"RGB scale : \",state.shape)\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "state = env.reset()\n",
    "print(\"Gray scale:\",state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLTHFpQXldnX"
   },
   "source": [
    ">Finally, it is important to group the frames when training. If you only train with one frame the AI will not be able to know where Mario or the enemies are moviing. This is why a FrameStack of 4 frames is created for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tPaZPDisleuy"
   },
   "outputs": [],
   "source": [
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yosni9AKlxe_"
   },
   "source": [
    ">This is the final pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HWcJ4YLkh1Xh"
   },
   "outputs": [],
   "source": [
    "MOVEMENT = [['left', 'A'], ['right', 'B'], ['right', 'A', 'B']]\n",
    "env = gym_super_mario_bros.make(STAGE_NAME)\n",
    "env = JoypadSpace(env, MOVEMENT)\n",
    "env = CustomRewardAndDoneEnv(env)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = ResizeEnv(env, size=84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QFWwhsBml0Jm",
    "outputId": "5325afb2-9ba4-496f-a291-2f4764b56816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "state, reward, done, info = env.step([0])\n",
    "print('state:', state.shape) #Color scale, height, width, num of stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KL_uSrTVl-kf"
   },
   "outputs": [],
   "source": [
    "def display_all_frame():\n",
    "    plt.figure(figsize=(16,16))\n",
    "    for idx in range(state.shape[3]):\n",
    "        plt.subplot(1,4,idx+1)\n",
    "        plt.imshow(state[0][:,:,idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "95YVJ49xmAyW",
    "outputId": "789080b0-a34e-4ede-ad3a-eea3015962cd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHPfiqJimJNn"
   },
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_QPID_iJIAQK"
   },
   "outputs": [],
   "source": [
    "# Import PPO for algos\n",
    "from stable_baselines3 import PPO\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fetFG9Y-KkVn"
   },
   "outputs": [],
   "source": [
    "# Model Param\n",
    "CHECK_FREQ_NUMB = 10000\n",
    "TOTAL_TIMESTEP_NUMB = 5000000\n",
    "LEARNING_RATE = 0.0001\n",
    "GAE = 1.0\n",
    "ENT_COEF = 0.01\n",
    "N_STEPS = 512\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Test Param\n",
    "EPISODE_NUMBERS = 20\n",
    "MAX_TIMESTEP_TEST = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSuCif-GitBJ"
   },
   "source": [
    ">Once the environment has been preprocessed, it is time to start training our AI model. In this case the stable-baseline3 PPO algorithm will be used due to its simplicity, but other alternatives such as DQN or DDQN can be explored. Before starting with the training, a convolutional neural network (CNN) has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWFOd0aYgz4L"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eKYEsKiHKChE"
   },
   "outputs": [],
   "source": [
    "class MarioNet(BaseFeaturesExtractor):\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim):\n",
    "        super(MarioNet, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rznnR_7xizcQ"
   },
   "source": [
    ">The next step consists of the creation of a file where the AI will save the results obtained in each iteration. In this way, later we will be able to visualize graphically the learning of our model.\n",
    "\n",
    ">In this case, the average score, the average starting time and the best score obtained will be saved for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "elCC-AerKSPD"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "save_dir = Path('./model')\n",
    "save_dir.mkdir(parents=True, exist_ok = True)\n",
    "reward_log_path = (save_dir / 'reward_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lAX834MzKXWG"
   },
   "outputs": [],
   "source": [
    "with open(reward_log_path, 'a') as f:\n",
    "    print('timesteps,reward,best_reward', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wQPtcA5j1rr"
   },
   "source": [
    ">This callback function will be in charge of writing the aforementioned data to the file. This function will be executed automatically each time an iteration has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ydS_MlH9KYwd"
   },
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = (save_dir / 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "            total_reward = [0] * EPISODE_NUMBERS\n",
    "            total_time = [0] * EPISODE_NUMBERS\n",
    "            best_reward = 0\n",
    "\n",
    "            for i in range(EPISODE_NUMBERS):\n",
    "                state = env.reset()  # reset for each new trial\n",
    "                done = False\n",
    "                total_reward[i] = 0\n",
    "                total_time[i] = 0\n",
    "                while not done and total_time[i] < MAX_TIMESTEP_TEST:\n",
    "                    action, _ = model.predict(state)\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                    total_reward[i] += reward[0]\n",
    "                    total_time[i] += 1\n",
    "\n",
    "                if total_reward[i] > best_reward:\n",
    "                    best_reward = total_reward[i]\n",
    "                    best_epoch = self.n_calls\n",
    "\n",
    "                state = env.reset()  # reset for each new trial\n",
    "\n",
    "            print('time steps:', self.n_calls, '/', TOTAL_TIMESTEP_NUMB)\n",
    "            print('average reward:', (sum(total_reward) / EPISODE_NUMBERS),\n",
    "                  'average time:', (sum(total_time) / EPISODE_NUMBERS),\n",
    "                  'best_reward:', best_reward)\n",
    "\n",
    "            with open(reward_log_path, 'a') as f:\n",
    "                print(self.n_calls, ',', sum(total_reward) / EPISODE_NUMBERS, ',', best_reward, file=f)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxx7434mkYN2"
   },
   "source": [
    ">Finally, all that remains is for our AI to start learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5yXiD4toLChp"
   },
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=CHECK_FREQ_NUMB, save_path=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qYFzPiWPLG5P"
   },
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, verbose=0, policy_kwargs=policy_kwargs, tensorboard_log=save_dir, learning_rate=LEARNING_RATE, n_steps=N_STEPS,\n",
    "              batch_size=BATCH_SIZE, n_epochs=N_EPOCHS, gamma=GAMMA, gae_lambda=GAE, ent_coef=ENT_COEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1zwA7R2zLIy5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps: 10000 / 5000000\n",
      "average reward: 249.2249962452799 average time: 136.1 best_reward: 420.09999468177557\n",
      "GOAL\n",
      "time steps: 20000 / 5000000\n",
      "average reward: 262.0249958205968 average time: 121.05 best_reward: 726.6999883502722\n",
      "time steps: 30000 / 5000000\n",
      "average reward: 209.60999659374357 average time: 88.35 best_reward: 491.6999917551875\n",
      "GOAL\n",
      "time steps: 40000 / 5000000\n",
      "average reward: 193.02499695941805 average time: 91.75 best_reward: 529.3999909907579\n",
      "GOAL\n",
      "time steps: 50000 / 5000000\n",
      "average reward: 212.0449962966144 average time: 91.3 best_reward: 448.49999161064625\n",
      "time steps: 60000 / 5000000\n",
      "average reward: 250.54999570287765 average time: 101.45 best_reward: 525.2999902442098\n",
      "time steps: 70000 / 5000000\n",
      "average reward: 216.77999641261994 average time: 102.0 best_reward: 529.4999894499779\n",
      "time steps: 80000 / 5000000\n",
      "average reward: 193.1899966966361 average time: 83.2 best_reward: 531.4999916180968\n",
      "time steps: 90000 / 5000000\n",
      "average reward: 286.48999476097526 average time: 119.4 best_reward: 598.8999891877174\n",
      "time steps: 100000 / 5000000\n",
      "average reward: 235.55499597154557 average time: 100.9 best_reward: 525.799991324544\n",
      "time steps: 110000 / 5000000\n",
      "average reward: 288.4649948980659 average time: 118.1 best_reward: 728.3999870344996\n",
      "time steps: 120000 / 5000000\n",
      "average reward: 246.6599955942482 average time: 102.2 best_reward: 448.3999928161502\n",
      "time steps: 130000 / 5000000\n",
      "average reward: 259.4399954557419 average time: 115.05 best_reward: 529.2999898865819\n",
      "time steps: 140000 / 5000000\n",
      "average reward: 283.6449952345341 average time: 118.8 best_reward: 577.0999904125929\n",
      "time steps: 150000 / 5000000\n",
      "average reward: 242.46999596320092 average time: 102.15 best_reward: 596.0999904498458\n",
      "time steps: 160000 / 5000000\n",
      "average reward: 258.8799955420196 average time: 99.6 best_reward: 448.99999160319567\n",
      "time steps: 170000 / 5000000\n",
      "average reward: 264.95499579161407 average time: 111.65 best_reward: 492.59999241679907\n",
      "time steps: 180000 / 5000000\n",
      "average reward: 253.1649958088994 average time: 100.8 best_reward: 728.2999889701605\n",
      "time steps: 190000 / 5000000\n",
      "average reward: 209.85999686382712 average time: 87.0 best_reward: 491.69999281316996\n",
      "GOAL\n",
      "time steps: 200000 / 5000000\n",
      "average reward: 321.96499490141866 average time: 132.25 best_reward: 983.5999875739217\n",
      "time steps: 210000 / 5000000\n",
      "average reward: 290.12999522425235 average time: 110.3 best_reward: 728.8999884873629\n",
      "time steps: 220000 / 5000000\n",
      "average reward: 257.9549954097718 average time: 105.0 best_reward: 579.2999894618988\n",
      "time steps: 230000 / 5000000\n",
      "average reward: 262.19499551542106 average time: 106.25 best_reward: 530.699990093708\n",
      "time steps: 240000 / 5000000\n",
      "average reward: 337.3949941869825 average time: 139.35 best_reward: 421.09999241679907\n",
      "time steps: 250000 / 5000000\n",
      "average reward: 277.8849951170385 average time: 98.3 best_reward: 449.29999179393053\n",
      "time steps: 260000 / 5000000\n",
      "average reward: 239.77499581091107 average time: 91.7 best_reward: 420.9999931305647\n",
      "GOAL\n",
      "time steps: 270000 / 5000000\n",
      "average reward: 350.0549938544631 average time: 133.25 best_reward: 596.499988771975\n",
      "GOAL\n",
      "time steps: 280000 / 5000000\n",
      "average reward: 256.58499555177985 average time: 109.6 best_reward: 526.2999897301197\n",
      "GOAL\n",
      "time steps: 290000 / 5000000\n",
      "average reward: 338.79999406710266 average time: 154.1 best_reward: 976.9999837651849\n",
      "time steps: 300000 / 5000000\n",
      "average reward: 380.37499301843343 average time: 166.45 best_reward: 716.5999872088432\n",
      "time steps: 310000 / 5000000\n",
      "average reward: 339.4149939816445 average time: 123.55 best_reward: 529.6999908834696\n",
      "time steps: 320000 / 5000000\n",
      "average reward: 413.21499245390294 average time: 144.25 best_reward: 533.5999907106161\n",
      "time steps: 330000 / 5000000\n",
      "average reward: 347.6399939436466 average time: 324.25 best_reward: 726.0999862104654\n",
      "time steps: 340000 / 5000000\n",
      "average reward: 384.5449937041849 average time: 182.8 best_reward: 527.6999912858009\n",
      "GOAL\n",
      "time steps: 350000 / 5000000\n",
      "average reward: 465.3599913969636 average time: 167.8 best_reward: 985.1999839022756\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 360000 / 5000000\n",
      "average reward: 447.8949919193983 average time: 163.45 best_reward: 985.3999856486917\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 370000 / 5000000\n",
      "average reward: 568.3049898389727 average time: 197.4 best_reward: 986.1999856457114\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 380000 / 5000000\n",
      "average reward: 475.65499114133416 average time: 167.9 best_reward: 985.099981456995\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 390000 / 5000000\n",
      "average reward: 459.2049915064126 average time: 163.45 best_reward: 729.9999864473939\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 400000 / 5000000\n",
      "average reward: 364.52999330535533 average time: 142.7 best_reward: 814.7999858185649\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 410000 / 5000000\n",
      "average reward: 390.4249927096069 average time: 139.55 best_reward: 985.9999813586473\n",
      "time steps: 420000 / 5000000\n",
      "average reward: 460.4849916607142 average time: 172.6 best_reward: 728.2999868020415\n",
      "GOAL\n",
      "time steps: 430000 / 5000000\n",
      "average reward: 386.08999283835294 average time: 158.65 best_reward: 721.7999868616462\n",
      "time steps: 440000 / 5000000\n",
      "average reward: 313.9499944183975 average time: 148.1 best_reward: 643.7999890372157\n",
      "GOAL\n",
      "time steps: 450000 / 5000000\n",
      "average reward: 518.4099906299264 average time: 202.8 best_reward: 979.2999822050333\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 460000 / 5000000\n",
      "average reward: 506.9549904294312 average time: 276.4 best_reward: 985.4999836906791\n",
      "GOAL\n",
      "time steps: 470000 / 5000000\n",
      "average reward: 325.2099947080016 average time: 122.5 best_reward: 529.7999907135963\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 480000 / 5000000\n",
      "average reward: 490.704991350323 average time: 186.1 best_reward: 981.2999844625592\n",
      "time steps: 490000 / 5000000\n",
      "average reward: 431.2149925932288 average time: 200.35 best_reward: 815.2999848946929\n",
      "time steps: 500000 / 5000000\n",
      "average reward: 426.85999195240436 average time: 267.05 best_reward: 729.999986961484\n",
      "time steps: 510000 / 5000000\n",
      "average reward: 482.3499908719212 average time: 345.15 best_reward: 643.8999867290258\n",
      "time steps: 520000 / 5000000\n",
      "average reward: 353.5649933643639 average time: 205.7 best_reward: 729.6999862268567\n",
      "GOAL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOTAL_TIMESTEP_NUMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:310\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:247\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    243\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 247\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:181\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    180\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/callbacks.py:88\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# timesteps start at zero\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36mTrainAndLoggingCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m total_time[i] \u001b[38;5;241m<\u001b[39m MAX_TIMESTEP_TEST:\n\u001b[1;32m     26\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(state)\n\u001b[0;32m---> 27\u001b[0m     state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     total_reward[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m     total_time[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:48\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]], np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],]:\n\u001b[0;32m---> 48\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     observations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackedobs\u001b[38;5;241m.\u001b[39mupdate(observations, dones, infos)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mSkipFrame.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      8\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[0;32m---> 10\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mCustomRewardAndDoneEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 15\u001b[0m     state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_pos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_x)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_pos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nes_py/wrappers/joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nes_py/nes_env.py:310\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_did_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# bound the reward in [min, max]\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m<\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_range\u001b[49m[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    311\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_range[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reward \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_range[\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4Z0cWQkeHA"
   },
   "source": [
    "## Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvWiZ6SFw3aY"
   },
   "source": [
    ">This last section analyzes the results and conclusions of this project. As can be seen in the graphs, two different models have been trained, one using the standard set and the other using the rectangle set.\n",
    "\n",
    ">In the standard game, 1050000 iteractions have been executed, while in the rectangular game there have been 640000.  Although the rectangular model has been trained with much fewer iterations, the best model has similar results to the best standard model. \n",
    "\n",
    ">If we run the function that calculates the win rate we can see that both models have a 20% win rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sbfGzJzfpzZ8",
    "outputId": "a689b59c-ebe0-4d67-a461-8c0a1528cc0c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reward_log_Standar.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m reward_log \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreward_log_Standar.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimesteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m reward_log\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reward_log_Standar.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reward_log = pd.read_csv(\"reward_log_Standar.csv\", index_col='timesteps')\n",
    "reward_log.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBWZxY3SqbSI",
    "outputId": "67a3f525-ec61-49cf-ff6b-8d9468b7ee54"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reward_log = pd.read_csv(\"reward_log_Rectangle.csv\", index_col='timesteps')\n",
    "reward_log.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPm7fr1npz5F",
    "outputId": "17dff4d4-50a9-4d48-8502-6a25d14b658d"
   },
   "outputs": [],
   "source": [
    "reward_log = pd.read_csv(\"reward_log_Standar.csv\", index_col='timesteps')\n",
    "best_epoch = reward_log['reward'].idxmax()\n",
    "print('best epoch:', best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzIoR_Ndp3AJ"
   },
   "outputs": [],
   "source": [
    "best_model_path = os.path.join(save_dir, 'best_model_{}'.format(best_epoch))\n",
    "model = PPO.load(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoToaDDVp46c"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = True\n",
    "plays = 0;\n",
    "wins = 0;\n",
    "while plays < 100:\n",
    "    if done:\n",
    "        state = env.reset() \n",
    "        if info[0][\"flag_get\"]:\n",
    "          wins += 1\n",
    "        plays += 1\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "print(\"Model win rate: \" + str(wins) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7dSVTueyhwo"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "while plays < 100:\n",
    "    if done:\n",
    "        state = env.reset() \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    #env.render() #Only in local, not in Colab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mctsDSbNzQXH"
   },
   "source": [
    ">[Demo](https://youtube.com/shorts/jta7SegNNwM)\n",
    "<iframe width='560' height='315' src=\"https://youtube.com/shorts/jta7SegNNwM\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2993870,
     "sourceId": 5152646,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30407,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
